{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92edc23ca83c46168820e47fabfc4107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_116dae3680e547f0a8ab3c75921becb1",
              "IPY_MODEL_abdedd92b4df4312ade23e1be5713824",
              "IPY_MODEL_a66f2e9459d14355ba36ff58cfade75a"
            ],
            "layout": "IPY_MODEL_f97e54a914df49719819b53e2bee11a3"
          }
        },
        "116dae3680e547f0a8ab3c75921becb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad311f971f584603bd751fba39fd92b3",
            "placeholder": "​",
            "style": "IPY_MODEL_5773f45773b444ca8bce694f135e0f8c",
            "value": "config.json: 100%"
          }
        },
        "abdedd92b4df4312ade23e1be5713824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afa30b27652b4448802896ae6c4fc64b",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90a1772a45804c99a529e542478a4a84",
            "value": 665
          }
        },
        "a66f2e9459d14355ba36ff58cfade75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a91d4137414a44f38b8f9254d6898c13",
            "placeholder": "​",
            "style": "IPY_MODEL_e2a9cf1aeedf4387aaa84606405d7dc8",
            "value": " 665/665 [00:00&lt;00:00, 9.56kB/s]"
          }
        },
        "f97e54a914df49719819b53e2bee11a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad311f971f584603bd751fba39fd92b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5773f45773b444ca8bce694f135e0f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afa30b27652b4448802896ae6c4fc64b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a1772a45804c99a529e542478a4a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a91d4137414a44f38b8f9254d6898c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2a9cf1aeedf4387aaa84606405d7dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f2a485b0786453cb107117e9de71a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9a2e4fc14694975aded3c3f6cf57945",
              "IPY_MODEL_7772b267f49f4311803dbed32c9bb59a",
              "IPY_MODEL_6646a993e4164812a3f3174c0bb177b0"
            ],
            "layout": "IPY_MODEL_5149844b48b0422eafadd7090b0bdd3b"
          }
        },
        "c9a2e4fc14694975aded3c3f6cf57945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_848206eba4094fb2b50b840d44e849f9",
            "placeholder": "​",
            "style": "IPY_MODEL_5faad2e6c1634d4f8d6374641df6a77b",
            "value": "model.safetensors: 100%"
          }
        },
        "7772b267f49f4311803dbed32c9bb59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d8e125b105a419db6eb599b190cafa5",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efa2d50f799f492da289c372b2654223",
            "value": 548105171
          }
        },
        "6646a993e4164812a3f3174c0bb177b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d6bbc5e44141f08b2fd2ef4995fb18",
            "placeholder": "​",
            "style": "IPY_MODEL_2c2da7d9bca944199affe3f62176eb2e",
            "value": " 548M/548M [00:06&lt;00:00, 26.2MB/s]"
          }
        },
        "5149844b48b0422eafadd7090b0bdd3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848206eba4094fb2b50b840d44e849f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5faad2e6c1634d4f8d6374641df6a77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d8e125b105a419db6eb599b190cafa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efa2d50f799f492da289c372b2654223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8d6bbc5e44141f08b2fd2ef4995fb18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c2da7d9bca944199affe3f62176eb2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e09b85b8e09f4dbd9e022074c502253c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbdbc32d8d054b1d807a84f04738c97d",
              "IPY_MODEL_5ce492ecb6284d868ab5e48585ebd4b8",
              "IPY_MODEL_f27fbf879ec64f9f8f8f7823af99edf5"
            ],
            "layout": "IPY_MODEL_1b816715e556482dbb1dbcd04d3331f7"
          }
        },
        "bbdbc32d8d054b1d807a84f04738c97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e43eff9c88e4295bd53165fcf374eda",
            "placeholder": "​",
            "style": "IPY_MODEL_805b4b1d379b4d3892ab871f262e5b1f",
            "value": "generation_config.json: 100%"
          }
        },
        "5ce492ecb6284d868ab5e48585ebd4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a147671cc2c4b0e98cc50a92eb4b4ed",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31af99a86c4e4ddf852a25ad34f60229",
            "value": 124
          }
        },
        "f27fbf879ec64f9f8f8f7823af99edf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7c6c9e353cc48969212383d0547d671",
            "placeholder": "​",
            "style": "IPY_MODEL_bf45c3d186754b668d5162d4c4885a9b",
            "value": " 124/124 [00:00&lt;00:00, 2.17kB/s]"
          }
        },
        "1b816715e556482dbb1dbcd04d3331f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e43eff9c88e4295bd53165fcf374eda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805b4b1d379b4d3892ab871f262e5b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a147671cc2c4b0e98cc50a92eb4b4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31af99a86c4e4ddf852a25ad34f60229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7c6c9e353cc48969212383d0547d671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf45c3d186754b668d5162d4c4885a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83551daeb4ad47668b21de330fb0f29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_147594a08278410b8bb7cbcb11042424",
              "IPY_MODEL_c89152ebb7bb47f684101ef9d5e1c3cd",
              "IPY_MODEL_886ad7a0421f48f3bb9bda2e7054441f"
            ],
            "layout": "IPY_MODEL_ee26cc2b33274710be2613517031fcba"
          }
        },
        "147594a08278410b8bb7cbcb11042424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3dda8a6fde14a07841a1cce6719d29c",
            "placeholder": "​",
            "style": "IPY_MODEL_b926ea5c575b4756a04be992592f5d85",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c89152ebb7bb47f684101ef9d5e1c3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f4934404da43babaa2fff96833693f",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c208df32d8541449ea33dbfbb0e28dd",
            "value": 26
          }
        },
        "886ad7a0421f48f3bb9bda2e7054441f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b80c725896a44b138515b811df0befa9",
            "placeholder": "​",
            "style": "IPY_MODEL_7f8655a2bd1142f5a0b38b95d9094acb",
            "value": " 26.0/26.0 [00:00&lt;00:00, 784B/s]"
          }
        },
        "ee26cc2b33274710be2613517031fcba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3dda8a6fde14a07841a1cce6719d29c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b926ea5c575b4756a04be992592f5d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43f4934404da43babaa2fff96833693f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c208df32d8541449ea33dbfbb0e28dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b80c725896a44b138515b811df0befa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f8655a2bd1142f5a0b38b95d9094acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "241c0b6800f94f1fb8d5aea32de8ffe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_becaf0b705ce4fb89c570674d954b486",
              "IPY_MODEL_a5480616253e458b9b01cf3d9bd56c6a",
              "IPY_MODEL_e87e01449a644b1d9aa46ea1b1327136"
            ],
            "layout": "IPY_MODEL_7a31045e259243029a2eeaa5fefdd3c3"
          }
        },
        "becaf0b705ce4fb89c570674d954b486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_015801c7d9174b1ea8141cf5cff128f1",
            "placeholder": "​",
            "style": "IPY_MODEL_2a6d7fd360734fcd88d717a3903ec2b4",
            "value": "vocab.json: 100%"
          }
        },
        "a5480616253e458b9b01cf3d9bd56c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1a5641ab955472bb7b89f95a41081bc",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86ae561d1e254d48b97e3badda9280c7",
            "value": 1042301
          }
        },
        "e87e01449a644b1d9aa46ea1b1327136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d72bf2daf5d3470cb3ae1f652a9d3e12",
            "placeholder": "​",
            "style": "IPY_MODEL_ef5fa9baf58d4dd3bf0b8d57e054ce19",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 7.12MB/s]"
          }
        },
        "7a31045e259243029a2eeaa5fefdd3c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "015801c7d9174b1ea8141cf5cff128f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a6d7fd360734fcd88d717a3903ec2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1a5641ab955472bb7b89f95a41081bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ae561d1e254d48b97e3badda9280c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d72bf2daf5d3470cb3ae1f652a9d3e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5fa9baf58d4dd3bf0b8d57e054ce19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44f40b07069f44ca9b1f19bc83dbea9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a465256544ed4a798ad51b393fd585ee",
              "IPY_MODEL_2e7fb71a41994dd6999ac568c4454ac9",
              "IPY_MODEL_99888a224fdb4bf9839c7fed81355594"
            ],
            "layout": "IPY_MODEL_da0d156a438b4cc1bf1da882fe6f21e7"
          }
        },
        "a465256544ed4a798ad51b393fd585ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d095b4049c5148fab121c80c0e83835c",
            "placeholder": "​",
            "style": "IPY_MODEL_26c48785a15f4c2f8cccce928aada785",
            "value": "merges.txt: 100%"
          }
        },
        "2e7fb71a41994dd6999ac568c4454ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e81b4473e6d246d6953567c1c2e31f00",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f75ade00a13467ab09ca7816aac4f11",
            "value": 456318
          }
        },
        "99888a224fdb4bf9839c7fed81355594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f79f81e82614083bfc356dd6cf34540",
            "placeholder": "​",
            "style": "IPY_MODEL_6e215493a5cd4f3b9168adef321e15c6",
            "value": " 456k/456k [00:00&lt;00:00, 7.95MB/s]"
          }
        },
        "da0d156a438b4cc1bf1da882fe6f21e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d095b4049c5148fab121c80c0e83835c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c48785a15f4c2f8cccce928aada785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e81b4473e6d246d6953567c1c2e31f00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f75ade00a13467ab09ca7816aac4f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f79f81e82614083bfc356dd6cf34540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e215493a5cd4f3b9168adef321e15c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd9546e483974754909315f88d6436ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_595c49781ce342ddad6d3dec9ce3cc56",
              "IPY_MODEL_61882094b18a441ebdb3335682ffd514",
              "IPY_MODEL_ef94a6c615764b669b101764a20ccdaf"
            ],
            "layout": "IPY_MODEL_3ffab69e4f07410aa296b933f40055b4"
          }
        },
        "595c49781ce342ddad6d3dec9ce3cc56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8357ea14d9bd4533a80da63102da53d3",
            "placeholder": "​",
            "style": "IPY_MODEL_82cf5a2916e74da797136a8c5a4e969f",
            "value": "tokenizer.json: 100%"
          }
        },
        "61882094b18a441ebdb3335682ffd514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a315d08c2d694e0abe964cf1cbd5785e",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d52c49d474164060bbb6c9ca8833579a",
            "value": 1355256
          }
        },
        "ef94a6c615764b669b101764a20ccdaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c54486e868844ea7945e81a5ec06fc5c",
            "placeholder": "​",
            "style": "IPY_MODEL_c68c48939d094181a690f35ab54b65f2",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 15.5MB/s]"
          }
        },
        "3ffab69e4f07410aa296b933f40055b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8357ea14d9bd4533a80da63102da53d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82cf5a2916e74da797136a8c5a4e969f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a315d08c2d694e0abe964cf1cbd5785e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52c49d474164060bbb6c9ca8833579a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c54486e868844ea7945e81a5ec06fc5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c68c48939d094181a690f35ab54b65f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install wandb\n",
        "!pip install llama-recipes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzmU28qvPdnO",
        "outputId": "2b46b58b-9b0e-4038-cd5b-8f4d1821de15"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n",
            "Collecting llama-recipes\n",
            "  Downloading llama_recipes-0.0.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (0.34.2)\n",
            "Collecting appdirs (from llama-recipes)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting bitsandbytes (from llama-recipes)\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting black (from llama-recipes)\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (5.2.0)\n",
            "Collecting codeshield (from llama-recipes)\n",
            "  Downloading codeshield-1.0.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (3.0.0)\n",
            "Collecting fire (from llama-recipes)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio (from llama-recipes)\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting loralib (from llama-recipes)\n",
            "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (3.7.1)\n",
            "Collecting openai (from llama-recipes)\n",
            "  Downloading openai-1.46.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting optimum (from llama-recipes)\n",
            "  Downloading optimum-1.22.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting peft (from llama-recipes)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting py7zr (from llama-recipes)\n",
            "  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (0.1.99)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (0.9.0)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.43.1 in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (4.44.2)\n",
            "Collecting typing-extensions==4.8.0 (from llama-recipes)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->llama-recipes) (3.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->llama-recipes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->llama-recipes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->llama-recipes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->llama-recipes) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->llama-recipes) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->llama-recipes) (5.9.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->llama-recipes)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->llama-recipes)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (4.3.4)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (2.0.1)\n",
            "Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]->llama-recipes) (7.34.0)\n",
            "Collecting tokenize-rt>=3.2.0 (from black[jupyter]->llama-recipes)\n",
            "  Downloading tokenize_rt-6.0.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semgrep>1.68 (from codeshield->llama-recipes)\n",
            "  Downloading semgrep-1.88.0-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (3.10.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llama-recipes) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llama-recipes) (2.4.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->llama-recipes)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->llama-recipes) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio->llama-recipes)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio->llama-recipes)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio->llama-recipes)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio->llama-recipes)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->llama-recipes) (6.4.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->llama-recipes) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio->llama-recipes)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->llama-recipes) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->llama-recipes) (2.9.2)\n",
            "Collecting pydub (from gradio->llama-recipes)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio->llama-recipes)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio->llama-recipes)\n",
            "  Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio->llama-recipes)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio->llama-recipes)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio->llama-recipes) (0.12.5)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->llama-recipes) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->llama-recipes)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio->llama-recipes)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->llama-recipes) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->llama-recipes) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->llama-recipes) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->llama-recipes) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->llama-recipes) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->llama-recipes) (2.8.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->llama-recipes) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai->llama-recipes)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->llama-recipes) (1.3.1)\n",
            "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openai (from llama-recipes)\n",
            "  Downloading openai-1.46.0-py3-none-any.whl.metadata (24 kB)\n",
            "  Downloading openai-1.45.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.45.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.44.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.43.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.41.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.41.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.8-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.7-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.5-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.4-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.3-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.2-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.40.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading openai-1.39.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting coloredlogs (from optimum->llama-recipes)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting texttable (from py7zr->llama-recipes)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr->llama-recipes)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pyzstd>=0.15.9 (from py7zr->llama-recipes)\n",
            "  Downloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr->llama-recipes)\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->llama-recipes)\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr->llama-recipes)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->llama-recipes)\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting brotli>=1.1.0 (from py7zr->llama-recipes)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->llama-recipes) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->llama-recipes) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio->llama-recipes)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->llama-recipes) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio->llama-recipes)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio->llama-recipes)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=7.8.0->black[jupyter]->llama-recipes)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]->llama-recipes) (4.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->llama-recipes) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->llama-recipes) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->llama-recipes) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->llama-recipes) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.43.1->llama-recipes) (3.3.2)\n",
            "Collecting boltons~=21.0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading boltons-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting click-option-group~=0.5 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading click_option_group-0.5.6-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting colorama~=0.4.0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: defusedxml~=0.7.1 in /usr/local/lib/python3.10/dist-packages (from semgrep>1.68->codeshield->llama-recipes) (0.7.1)\n",
            "Collecting glom~=22.1 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading glom-22.1.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: jsonschema~=4.6 in /usr/local/lib/python3.10/dist-packages (from semgrep>1.68->codeshield->llama-recipes) (4.23.0)\n",
            "Collecting opentelemetry-api~=1.25.0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-sdk~=1.25.0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http~=1.25.0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_instrumentation_requests-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: peewee~=3.14 in /usr/local/lib/python3.10/dist-packages (from semgrep>1.68->codeshield->llama-recipes) (3.17.6)\n",
            "Collecting rich~=13.5.2 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting ruamel.yaml<0.18,>=0.16.0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading ruamel.yaml-0.17.40-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting wcmatch~=8.3 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading wcmatch-8.5.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.45.0,>=4.29->optimum->llama-recipes) (3.20.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->llama-recipes) (1.5.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum->llama-recipes)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2->llama-recipes) (1.3.0)\n",
            "Collecting face>=20.1.0 (from glom~=22.1->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading face-22.0.0-py3-none-any.whl.metadata (816 bytes)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]->llama-recipes) (0.8.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema~=4.6->semgrep>1.68->codeshield->llama-recipes) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema~=4.6->semgrep>1.68->codeshield->llama-recipes) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema~=4.6->semgrep>1.68->codeshield->llama-recipes) (0.20.0)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api~=1.25.0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api~=1.25.0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http~=1.25.0->semgrep>1.68->codeshield->llama-recipes) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-http~=1.25.0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-http~=1.25.0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes) (1.16.0)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-semantic-conventions to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_instrumentation_requests-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_instrumentation_requests-0.46b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]->llama-recipes) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black[jupyter]->llama-recipes) (0.2.13)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich~=13.5.2->semgrep>1.68->codeshield->llama-recipes) (3.0.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml<0.18,>=0.16.0->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting bracex>=2.1.1 (from wcmatch~=8.3->semgrep>1.68->codeshield->llama-recipes)\n",
            "  Downloading bracex-2.5-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api~=1.25.0->semgrep>1.68->codeshield->llama-recipes) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich~=13.5.2->semgrep>1.68->codeshield->llama-recipes) (0.1.2)\n",
            "Downloading llama_recipes-0.0.3-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading codeshield-1.0.1-py3-none-any.whl (173 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
            "Downloading openai-1.39.0-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.7/336.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.22.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.7/453.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.8/413.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading semgrep-1.88.0-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (27.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenize_rt-6.0.0-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading boltons-21.0.0-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_option_group-0.5.6-py3-none-any.whl (12 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading glom-22.1.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.7/100.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_requests-0.46b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.5.3-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.8/239.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.17.40-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wcmatch-8.5.2-py3-none-any.whl (39 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bracex-2.5-py3-none-any.whl (11 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading face-22.0.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=39711d2872809c70559fede5684d78a50377efd0e728be613780431060bbaa25\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: texttable, pydub, brotli, boltons, appdirs, websockets, typing-extensions, tomlkit, tokenize-rt, semantic-version, ruff, ruamel.yaml.clib, pyzstd, python-multipart, pyppmd, pycryptodomex, pybcj, pathspec, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, multivolumefile, loralib, jedi, inflate64, importlib-metadata, humanfriendly, h11, fire, ffmpy, face, deprecated, colorama, click-option-group, bracex, aiofiles, wcmatch, uvicorn, starlette, ruamel.yaml, rich, py7zr, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpcore, glom, coloredlogs, black, opentelemetry-semantic-conventions, opentelemetry-instrumentation, httpx, bitsandbytes, opentelemetry-sdk, opentelemetry-instrumentation-requests, openai, gradio-client, fastapi, peft, opentelemetry-exporter-otlp-proto-http, gradio, semgrep, optimum, codeshield, llama-recipes\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.8.1\n",
            "    Uninstalling rich-13.8.1:\n",
            "      Successfully uninstalled rich-13.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "pymc 5.16.2 requires rich>=13.7.1, but you have rich 13.5.3 which is incompatible.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 appdirs-1.4.4 bitsandbytes-0.43.3 black-24.8.0 boltons-21.0.0 bracex-2.5 brotli-1.1.0 click-option-group-0.5.6 codeshield-1.0.1 colorama-0.4.6 coloredlogs-15.0.1 deprecated-1.2.14 face-22.0.0 fastapi-0.115.0 ffmpy-0.4.0 fire-0.6.0 glom-22.1.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 humanfriendly-10.0 importlib-metadata-7.1.0 inflate64-1.0.0 jedi-0.19.1 llama-recipes-0.0.3 loralib-0.1.2 multivolumefile-0.2.3 mypy-extensions-1.0.0 openai-1.39.0 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-http-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-requests-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 optimum-1.22.0 orjson-3.10.7 pathspec-0.12.1 peft-0.12.0 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.20.0 pydub-0.25.1 pyppmd-1.1.0 python-multipart-0.0.9 pyzstd-0.16.1 rich-13.5.3 ruamel.yaml-0.17.40 ruamel.yaml.clib-0.2.8 ruff-0.6.5 semantic-version-2.10.0 semgrep-1.88.0 starlette-0.38.5 texttable-1.7.0 tokenize-rt-6.0.0 tomlkit-0.12.0 typing-extensions-4.8.0 uvicorn-0.30.6 wcmatch-8.5.2 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NgZArlsnNlCQ"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y5uibpwO5FY",
        "outputId": "f14072e7-ed66-46f8-8d47-1a768ead3cff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"openai-community/gpt2\",\n",
        "    device_map=\"cuda\",\n",
        "    use_cache=None,\n",
        "    attn_implementation=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "92edc23ca83c46168820e47fabfc4107",
            "116dae3680e547f0a8ab3c75921becb1",
            "abdedd92b4df4312ade23e1be5713824",
            "a66f2e9459d14355ba36ff58cfade75a",
            "f97e54a914df49719819b53e2bee11a3",
            "ad311f971f584603bd751fba39fd92b3",
            "5773f45773b444ca8bce694f135e0f8c",
            "afa30b27652b4448802896ae6c4fc64b",
            "90a1772a45804c99a529e542478a4a84",
            "a91d4137414a44f38b8f9254d6898c13",
            "e2a9cf1aeedf4387aaa84606405d7dc8",
            "7f2a485b0786453cb107117e9de71a09",
            "c9a2e4fc14694975aded3c3f6cf57945",
            "7772b267f49f4311803dbed32c9bb59a",
            "6646a993e4164812a3f3174c0bb177b0",
            "5149844b48b0422eafadd7090b0bdd3b",
            "848206eba4094fb2b50b840d44e849f9",
            "5faad2e6c1634d4f8d6374641df6a77b",
            "1d8e125b105a419db6eb599b190cafa5",
            "efa2d50f799f492da289c372b2654223",
            "c8d6bbc5e44141f08b2fd2ef4995fb18",
            "2c2da7d9bca944199affe3f62176eb2e",
            "e09b85b8e09f4dbd9e022074c502253c",
            "bbdbc32d8d054b1d807a84f04738c97d",
            "5ce492ecb6284d868ab5e48585ebd4b8",
            "f27fbf879ec64f9f8f8f7823af99edf5",
            "1b816715e556482dbb1dbcd04d3331f7",
            "5e43eff9c88e4295bd53165fcf374eda",
            "805b4b1d379b4d3892ab871f262e5b1f",
            "1a147671cc2c4b0e98cc50a92eb4b4ed",
            "31af99a86c4e4ddf852a25ad34f60229",
            "a7c6c9e353cc48969212383d0547d671",
            "bf45c3d186754b668d5162d4c4885a9b"
          ]
        },
        "id": "pFrUKGwHP2Ss",
        "outputId": "6db9b08f-7d13-4cbf-b85d-db3ad31f3069"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92edc23ca83c46168820e47fabfc4107"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f2a485b0786453cb107117e9de71a09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e09b85b8e09f4dbd9e022074c502253c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VK0t4ncbwUp",
        "outputId": "c5cc0f68-9bca-43a7-d715-5365d7ce8664"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "83551daeb4ad47668b21de330fb0f29d",
            "147594a08278410b8bb7cbcb11042424",
            "c89152ebb7bb47f684101ef9d5e1c3cd",
            "886ad7a0421f48f3bb9bda2e7054441f",
            "ee26cc2b33274710be2613517031fcba",
            "d3dda8a6fde14a07841a1cce6719d29c",
            "b926ea5c575b4756a04be992592f5d85",
            "43f4934404da43babaa2fff96833693f",
            "3c208df32d8541449ea33dbfbb0e28dd",
            "b80c725896a44b138515b811df0befa9",
            "7f8655a2bd1142f5a0b38b95d9094acb",
            "241c0b6800f94f1fb8d5aea32de8ffe2",
            "becaf0b705ce4fb89c570674d954b486",
            "a5480616253e458b9b01cf3d9bd56c6a",
            "e87e01449a644b1d9aa46ea1b1327136",
            "7a31045e259243029a2eeaa5fefdd3c3",
            "015801c7d9174b1ea8141cf5cff128f1",
            "2a6d7fd360734fcd88d717a3903ec2b4",
            "b1a5641ab955472bb7b89f95a41081bc",
            "86ae561d1e254d48b97e3badda9280c7",
            "d72bf2daf5d3470cb3ae1f652a9d3e12",
            "ef5fa9baf58d4dd3bf0b8d57e054ce19",
            "44f40b07069f44ca9b1f19bc83dbea9c",
            "a465256544ed4a798ad51b393fd585ee",
            "2e7fb71a41994dd6999ac568c4454ac9",
            "99888a224fdb4bf9839c7fed81355594",
            "da0d156a438b4cc1bf1da882fe6f21e7",
            "d095b4049c5148fab121c80c0e83835c",
            "26c48785a15f4c2f8cccce928aada785",
            "e81b4473e6d246d6953567c1c2e31f00",
            "8f75ade00a13467ab09ca7816aac4f11",
            "0f79f81e82614083bfc356dd6cf34540",
            "6e215493a5cd4f3b9168adef321e15c6",
            "bd9546e483974754909315f88d6436ca",
            "595c49781ce342ddad6d3dec9ce3cc56",
            "61882094b18a441ebdb3335682ffd514",
            "ef94a6c615764b669b101764a20ccdaf",
            "3ffab69e4f07410aa296b933f40055b4",
            "8357ea14d9bd4533a80da63102da53d3",
            "82cf5a2916e74da797136a8c5a4e969f",
            "a315d08c2d694e0abe964cf1cbd5785e",
            "d52c49d474164060bbb6c9ca8833579a",
            "c54486e868844ea7945e81a5ec06fc5c",
            "c68c48939d094181a690f35ab54b65f2"
          ]
        },
        "id": "XRtPruW-Qm-u",
        "outputId": "267ce478-a350-4211-9bc0-40a4470b3cb4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83551daeb4ad47668b21de330fb0f29d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "241c0b6800f94f1fb8d5aea32de8ffe2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44f40b07069f44ca9b1f19bc83dbea9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd9546e483974754909315f88d6436ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = '''A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?'''\n",
        "batch = tokenizer(user_prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "U1h517QQQ5W9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The batch keys are:\",batch.keys())\n",
        "print(\"The input_ids are:\",batch[\"input_ids\"])\n",
        "print(\"Then length of the input_ids are:\",len(batch[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGBrSa_4RHwb",
        "outputId": "1acb75d6-586b-4ae7-c480-d8ba151fc039"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The batch keys are: dict_keys(['input_ids', 'attention_mask'])\n",
            "The input_ids are: tensor([[   32, 33192,  2753,   362, 24667,   286,  4171, 13608,   290,  2063,\n",
            "           326,   881,  2330, 13608,    13,  1374,   867, 24667,   287,  2472,\n",
            "           857,   340,  1011,    30]])\n",
            "Then length of the input_ids are: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **batch,\n",
        "        max_new_tokens=50, #The maximum numbers of tokens to generate\n",
        "        do_sample=False, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
        "        top_p=1.0, # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
        "        temperature=0, # [optional] The value used to modulate the next token probabilities.\n",
        "        min_length=None, #The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
        "        use_cache=True, #[optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
        "        top_k=50, # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "        repetition_penalty=1.0, #The parameter for repetition penalty. 1.0 means no penalty.\n",
        "        length_penalty=1, #[optional] Exponential penalty to the length that is used with beam-based generation.\n",
        "        output_hidden_states= True, return_dict_in_generate=True,\n",
        "    )\n",
        "batch = {k: v.to(\"cpu\") for k, v in batch.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db4UZ8ooRQda",
        "outputId": "26c587ac-e5c0-4ef6-f878-f17f20c4b64d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "peQ0VeiTSEZW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The complete decode:\",output_text)\n",
        "print(\"--------------------------------\")\n",
        "print(\"Just the model's response\",output_text[len(user_prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA_77d-YSfAT",
        "outputId": "e1069f91-779f-4c4d-e6c4-25dff354d23b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The complete decode: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\n",
            "\n",
            "The answer is that it takes about 1.5 bolts of blue fiber.\n",
            "\n",
            "The reason for this is that the blue fiber is not as dense as the white fiber.\n",
            "\n",
            "The reason for this is that the blue fiber is not\n",
            "--------------------------------\n",
            "Just the model's response \n",
            "\n",
            "The answer is that it takes about 1.5 bolts of blue fiber.\n",
            "\n",
            "The reason for this is that the blue fiber is not as dense as the white fiber.\n",
            "\n",
            "The reason for this is that the blue fiber is not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "RyJfY_x9TLsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import wandb\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import default_data_collator\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from contextlib import nullcontext\n",
        "from datetime import datetime\n",
        "from llama_recipes.utils.memory_utils import MemoryTrace\n",
        "import time\n",
        "import os\n",
        "import contextlib\n",
        "import json\n",
        "from transformers.data import DataCollatorForSeq2Seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNyWRr8JUANe",
        "outputId": "bb78381f-8138-4cd4-faf2-be08187d7943"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
            "  from torch.distributed._shard.checkpoint import (\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_json(output_filename, train_step_loss, train_epoch_loss, train_step_ppl, train_epoch_ppl, val_step_loss, val_epoch_loss, val_step_ppl, val_epoch_ppl):\n",
        "    metrics_data = {\n",
        "        \"train_step_loss\": train_step_loss,\n",
        "        \"train_epoch_loss\": train_epoch_loss,\n",
        "        \"train_step_perplexity\": train_step_ppl,\n",
        "        \"train_epoch_perplexity\": train_epoch_ppl,\n",
        "        \"val_step_loss\": val_step_loss,\n",
        "        \"val_epoch_loss\": val_epoch_loss,\n",
        "        \"val_step_perplexity\": val_step_ppl,\n",
        "        \"val_epoch_perplexity\": val_epoch_ppl\n",
        "    }\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        json.dump(metrics_data, f)"
      ],
      "metadata": {
        "id": "AbF2RulreNA9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LengthBasedBatchSampler(torch.utils.data.BatchSampler):\n",
        "    def __init__(self, data_source, batch_size: int, drop_last: bool, shuffle: bool=True) -> None:\n",
        "        if isinstance(next(iter(data_source)), dict):\n",
        "            first_key = next(iter(next(iter(data_source)).keys()))\n",
        "            self.lengths = [len(d[first_key]) for d in data_source]\n",
        "        else:\n",
        "            self.lengths = [len(d) for d in data_source]\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        ids = np.argsort(self.lengths, kind='mergesort')\n",
        "        if self.drop_last:\n",
        "            ids = ids[:len(ids) // self.batch_size * self.batch_size]\n",
        "\n",
        "        batches = [ids[i:i+self.batch_size] for i in range(0, len(ids), self.batch_size)]\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(batches)\n",
        "\n",
        "        for b in batches:\n",
        "            yield b\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.drop_last:\n",
        "            return len(self.lengths) // self.batch_size\n",
        "        else:\n",
        "            return len(self.lengths) // self.batch_size + (len(self.lengths) % self.batch_size > 0)"
      ],
      "metadata": {
        "id": "64QbCJ4OcfQh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader_kwargs(train_config, dataset, tokenizer, mode):\n",
        "        kwargs = {}\n",
        "        batch_size = train_config.batch_size_training if mode==\"train\" else train_config.val_batch_size\n",
        "        if train_config.batching_strategy == \"padding\":\n",
        "            kwargs[\"batch_sampler\"] = LengthBasedBatchSampler(dataset, batch_size, drop_last=True, shuffle=mode==\"train\")\n",
        "            kwargs[\"collate_fn\"] = DataCollatorForSeq2Seq(tokenizer)\n",
        "        elif train_config.batching_strategy == \"packing\":\n",
        "            kwargs[\"batch_size\"] = batch_size\n",
        "            kwargs[\"drop_last\"] = True\n",
        "            kwargs[\"collate_fn\"] = default_data_collator\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown batching strategy: {train_config.batching_strategy}\")\n",
        "\n",
        "        return kwargs"
      ],
      "metadata": {
        "id": "XoSRuqXLWjGe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@contextlib.contextmanager\n",
        "def profile(cfg, local_rank=None):\n",
        "    use_profiler: bool = cfg.use_profiler\n",
        "    use_flop_counter: bool = cfg.flop_counter\n",
        "    if use_flop_counter and use_profiler:\n",
        "        raise ValueError(\"Cannot use both profiler and flop counter\")\n",
        "    if use_profiler:\n",
        "        # profiler needs a warmup stage to get the accurate profiling results\n",
        "        wait_step, warmup_step, active_step = 1, 2, 3\n",
        "        min_step = wait_step + warmup_step + active_step + 1\n",
        "        if cfg.max_train_step > 0 and cfg.max_train_step < min_step:\n",
        "            raise ValueError(f\"pytorch profiler requires at least {min_step} train steps to finish the warm-up and recording stage, {wait_step} for wait_step, {warmup_step} for warmup_step, {active_step} for profiling step, please increase the max_train_step, current max_train_step {cfg.max_train_step}\")\n",
        "        print(f\"pytorch profiling is activated and results will be saved in {cfg.profiler_dir}\")\n",
        "        with torch.profiler.profile(\n",
        "            activities=[\n",
        "                torch.profiler.ProfilerActivity.CPU,\n",
        "                torch.profiler.ProfilerActivity.CUDA,\n",
        "            ],\n",
        "            schedule=torch.profiler.schedule(wait=wait_step, warmup=warmup_step, active=active_step, repeat=1),\n",
        "            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
        "                cfg.profiler_dir\n",
        "            ),\n",
        "            profile_memory=True,\n",
        "            with_stack=False,\n",
        "            with_flops=True,\n",
        "            record_shapes=True,\n",
        "        ) as torch_profiler:\n",
        "            yield torch_profiler\n",
        "    elif use_flop_counter:\n",
        "        if cfg.max_train_step > 0 and cfg.max_train_step <= cfg.flop_counter_start:\n",
        "            raise ValueError(f\"flop counter requires at least {cfg.flop_counter_start + 1} train steps, please increase the max_train_step, current max_train_step {cfg.max_train_step}\")\n",
        "        with FlopMeasure(rank=local_rank,warmup_step=cfg.flop_counter_start) as flop_counter:\n",
        "            yield flop_counter\n",
        "    else:\n",
        "        torch_profiler = contextlib.nullcontext()\n",
        "        yield None"
      ],
      "metadata": {
        "id": "cZ5r2gU_Yedo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatDataset(Dataset):\n",
        "    def __init__(self, dataset, chunk_size=4096):\n",
        "        self.dataset = dataset\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        self.samples = []\n",
        "        buffer = {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "            }\n",
        "\n",
        "        for sample in tqdm(self.dataset, desc=\"Preprocessing dataset\", dynamic_ncols=True):\n",
        "            buffer = {k: v + sample[k] for k,v in buffer.items()}\n",
        "\n",
        "            while len(next(iter(buffer.values()))) > self.chunk_size:\n",
        "                self.samples.append({k: v[:self.chunk_size] for k,v in buffer.items()})\n",
        "                buffer = {k: v[self.chunk_size:] for k,v in buffer.items()}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "metadata": {
        "id": "tpjrHRvlWUTJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, eval_dataloader, tokenizer, optimizer, lr_scheduler, gradient_accumulation_steps, train_config, wandb_run=None):\n",
        "    \"\"\"\n",
        "    Trains the model on the given dataloader\n",
        "\n",
        "    Args:\n",
        "        model: The model to be trained\n",
        "        train_dataloader: The dataloader containing the training data\n",
        "        optimizer: The optimizer used for training\n",
        "        lr_scheduler: The learning rate scheduler\n",
        "        gradient_accumulation_steps: The number of steps to accumulate gradients before performing a backward/update operation\n",
        "        num_epochs: The number of epochs to train for\n",
        "        local_rank: The rank of the current node in a distributed setting\n",
        "        train_config: The training configuration\n",
        "        eval_dataloader: The dataloader containing the eval data\n",
        "        tokenizer: tokenizer used in the eval for decoding the predicitons\n",
        "\n",
        "    Returns: results dictionary containing average training and validation perplexity and loss\n",
        "    \"\"\"\n",
        "\n",
        "    local_rank=None\n",
        "    rank=None\n",
        "\n",
        "    autocast = nullcontext\n",
        "    train_prep = []\n",
        "    train_loss = []\n",
        "    val_prep = []\n",
        "    val_loss =[]\n",
        "\n",
        "    if train_config.save_metrics:\n",
        "        metrics_filename = f\"{train_config.output_dir}/metrics_data_{local_rank}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
        "        train_step_perplexity = []\n",
        "        train_step_loss = []\n",
        "        val_step_loss = []\n",
        "        val_step_perplexity = []\n",
        "\n",
        "    epoch_times = []\n",
        "    checkpoint_times = []\n",
        "    results = {}\n",
        "    best_val_loss = float(\"inf\")\n",
        "    total_train_steps = 0\n",
        "    max_steps_reached = False  # Flag to indicate max training steps reached\n",
        "    # Start the training loop\n",
        "    for epoch in range(train_config.num_epochs):\n",
        "        # stop when the maximum number of training steps is reached\n",
        "\n",
        "        if max_steps_reached:\n",
        "            break\n",
        "        epoch_start_time = time.perf_counter()\n",
        "        with MemoryTrace() as memtrace:  # track the memory usage\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "            total_loss_f = 0.0\n",
        "            total_loss_m = 0.0\n",
        "            total_length = len(train_dataloader)//gradient_accumulation_steps\n",
        "            pbar = tqdm(colour=\"blue\", desc=f\"Training Epoch: {epoch+1}\", total=total_length, dynamic_ncols=True)\n",
        "            with profile(train_config,local_rank) as profile_context:\n",
        "                for step, batch in enumerate(train_dataloader):\n",
        "                    total_train_steps += 1\n",
        "                    # stop when the maximum number of training steps is reached\n",
        "                    if train_config.max_train_step > 0 and total_train_steps > train_config.max_train_step:\n",
        "                        max_steps_reached = True\n",
        "                        print(\"max training steps reached, stopping training, total train steps finished: \", total_train_steps-1)\n",
        "                        break\n",
        "                    # print(f\"The keys in the batch are:{batch.keys()}\")\n",
        "                    for key in batch.keys():\n",
        "                        # if is_xpu_available():\n",
        "                        #     batch[key] = batch[key].to('xpu:0')\n",
        "                        # else:\n",
        "                          batch[key] = batch[key].to('cuda:0')\n",
        "                    with autocast():\n",
        "                        outputs = model(**batch)\n",
        "                        loss = outputs.loss\n",
        "\n",
        "                    loss = loss / gradient_accumulation_steps\n",
        "\n",
        "                    if train_config.save_metrics:\n",
        "                        train_step_loss.append(loss.detach().float().item())\n",
        "                        train_step_perplexity.append(float(torch.exp(loss.detach().float())))\n",
        "\n",
        "                    total_loss += loss.detach().float()\n",
        "                    loss.backward()\n",
        "\n",
        "                    if (step + 1) % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
        "                        if train_config.gradient_clipping and train_config.gradient_clipping_threshold > 0.0:\n",
        "                            torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.gradient_clipping_threshold)\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                        pbar.update(1)\n",
        "                    if train_config.use_profiler or train_config.flop_counter:\n",
        "                        profile_context.step()\n",
        "                    if train_config.flop_counter and profile_context.is_done():\n",
        "                        TFlops = profile_context.get_flops_per_sec() / 1e12\n",
        "                    if wandb_run:\n",
        "                        wandb_run.log({\n",
        "                            'train/epoch': epoch + 1,\n",
        "                            'train/step': epoch * len(train_dataloader) + step,\n",
        "                            'train/loss': loss.detach().float(),\n",
        "                        })\n",
        "\n",
        "                    pbar.set_description(f\"Training Epoch: {epoch+1}/{train_config.num_epochs}, step {step}/{len(train_dataloader)} completed (loss: {loss.detach().float()})\")\n",
        "\n",
        "                    if step % 50 == 0:\n",
        "                        if train_config.run_validation:\n",
        "                            eval_ppl, eval_epoch_loss, temp_val_loss, temp_step_perplexity = evaluation(model, train_config, eval_dataloader, local_rank, tokenizer, wandb_run)\n",
        "                            if train_config.save_metrics:\n",
        "                                val_step_loss.extend(temp_val_loss)\n",
        "                                val_step_perplexity.extend(temp_step_perplexity)\n",
        "                        model.train()\n",
        "\n",
        "                    if train_config.save_metrics:\n",
        "                        save_to_json(metrics_filename, train_step_loss, train_loss, train_step_perplexity, train_prep, val_step_loss, val_loss, val_step_perplexity, val_prep)\n",
        "                pbar.close()\n",
        "\n",
        "        epoch_end_time = time.perf_counter()-epoch_start_time\n",
        "        epoch_times.append(epoch_end_time)\n",
        "        train_epoch_loss = total_loss / len(train_dataloader)\n",
        "        train_perplexity = torch.exp(train_epoch_loss)\n",
        "        train_prep.append(float(train_perplexity))\n",
        "        train_loss.append(float(train_epoch_loss))\n",
        "        memtrace.print_stats()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        if train_config.run_validation:\n",
        "            eval_ppl, eval_epoch_loss, temp_val_loss, temp_step_perplexity = evaluation(model, train_config, eval_dataloader, local_rank, tokenizer, wandb_run)\n",
        "            if train_config.save_metrics:\n",
        "                val_step_loss.extend(temp_val_loss)\n",
        "                val_step_perplexity.extend(temp_step_perplexity)\n",
        "\n",
        "            checkpoint_start_time = time.perf_counter()\n",
        "\n",
        "            if train_config.save_model :\n",
        "                epoch_dir = os.path.join(train_config.output_dir, f\"epoch{epoch}\")\n",
        "                os.makedirs(epoch_dir, exist_ok=True)\n",
        "                # Save the model in the new directory\n",
        "                model.save_pretrained(epoch_dir)\n",
        "                print(f\"Model is saved in {epoch_dir} directory\")\n",
        "\n",
        "            checkpoint_end_time = time.perf_counter() - checkpoint_start_time\n",
        "            checkpoint_times.append(checkpoint_end_time)\n",
        "            if eval_epoch_loss < best_val_loss:\n",
        "                best_val_loss = eval_epoch_loss\n",
        "                print(f\"best eval loss on epoch {epoch+1} is {best_val_loss}\")\n",
        "            val_loss.append(float(best_val_loss))\n",
        "            val_prep.append(float(eval_ppl))\n",
        "        print(f\"Epoch {epoch+1}: train_perplexity={train_perplexity:.4f}, train_epoch_loss={train_epoch_loss:.4f}, epoch time {epoch_end_time}s\")\n",
        "\n",
        "        # Saving the results every epoch to plot later\n",
        "        if train_config.save_metrics:\n",
        "            save_to_json(metrics_filename, train_step_loss, train_loss, train_step_perplexity, train_prep, val_step_loss, val_loss, val_step_perplexity, val_prep)\n",
        "\n",
        "    avg_epoch_time = sum(epoch_times)/ len(epoch_times)\n",
        "    avg_checkpoint_time = sum(checkpoint_times)/ len(checkpoint_times) if len(checkpoint_times) > 0 else 0\n",
        "    avg_train_prep = sum(train_prep)/len(train_prep)\n",
        "    avg_train_loss = sum(train_loss)/len(train_loss)\n",
        "    if train_config.run_validation:\n",
        "        avg_eval_prep = sum(val_prep)/len(val_prep)\n",
        "        avg_eval_loss = sum(val_loss)/len(val_loss)\n",
        "\n",
        "    results['avg_train_prep'] = avg_train_prep\n",
        "    results['avg_train_loss'] = avg_train_loss\n",
        "    if train_config.run_validation:\n",
        "        results['avg_eval_prep'] = avg_eval_prep\n",
        "        results['avg_eval_loss'] = avg_eval_loss\n",
        "    results[\"avg_epoch_time\"] = avg_epoch_time\n",
        "    results[\"avg_checkpoint_time\"] = avg_checkpoint_time\n",
        "    if train_config.save_metrics:\n",
        "        results[\"metrics_filename\"] = metrics_filename\n",
        "    return results"
      ],
      "metadata": {
        "id": "v3EQcIppXqRZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, train_config, eval_dataloader, local_rank, tokenizer, wandb_run):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader\n",
        "\n",
        "    Args:\n",
        "        model: The model to evaluate\n",
        "        eval_dataloader: The dataloader containing the evaluation data\n",
        "        local_rank: The rank of the current node in a distributed setting\n",
        "        tokenizer: The tokenizer used to decode predictions\n",
        "\n",
        "    Returns: eval_ppl, eval_epoch_loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    eval_preds = []\n",
        "    val_step_loss = []\n",
        "    val_step_perplexity = []\n",
        "    eval_loss = 0.0  # Initialize evaluation loss\n",
        "    eval_loss_f = 0.0\n",
        "    eval_loss_m = 0.0\n",
        "    total_eval_steps = 0\n",
        "    with MemoryTrace() as memtrace:\n",
        "        for step, batch in enumerate(tqdm(eval_dataloader,colour=\"green\", desc=\"evaluating Epoch\", dynamic_ncols=True)):\n",
        "            total_eval_steps += 1\n",
        "            # stop when the maximum number of eval steps is reached\n",
        "            if train_config.max_eval_step > 0 and total_eval_steps > train_config.max_eval_step:\n",
        "                break\n",
        "            for key in batch.keys():\n",
        "                # if is_xpu_available():\n",
        "                #     batch[key] = batch[key].to('xpu:0')\n",
        "                # else:\n",
        "                batch[key] = batch[key].to('cuda:0')\n",
        "            # Ensure no gradients are computed for this scope to save memory\n",
        "            with torch.no_grad():\n",
        "                # Forward pass and compute loss\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "                if train_config.save_metrics:\n",
        "                    val_step_loss.append(loss.detach().float().item())\n",
        "                    val_step_perplexity.append(float(torch.exp(loss.detach().float())))\n",
        "\n",
        "                eval_loss += loss.detach().float()\n",
        "            # Decode predictions and add to evaluation predictions list\n",
        "            preds = torch.argmax(outputs.logits, -1)\n",
        "            eval_preds.extend(\n",
        "                tokenizer.batch_decode(preds.detach().cpu().numpy(), skip_special_tokens=True)\n",
        "            )\n",
        "# Compute average loss and perplexity\n",
        "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
        "    eval_epoch_loss_f = eval_loss_f / len(eval_dataloader)\n",
        "    eval_epoch_loss_m = eval_loss_m / len(eval_dataloader)\n",
        "    eval_ppl = torch.exp(eval_epoch_loss)\n",
        "# Print evaluation metrics\n",
        "    if wandb_run:\n",
        "        wandb_run.log({\n",
        "                        'eval/perplexity': eval_ppl,\n",
        "                        'eval/loss': eval_epoch_loss,\n",
        "                    }, commit=False)\n",
        "    return eval_ppl, eval_epoch_loss, val_step_loss, val_step_perplexity"
      ],
      "metadata": {
        "id": "CCrymPbxZssT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preprocessed_dataset(tokenizer):\n",
        "    # Load dataset from Hugging Face\n",
        "    dataset = datasets.load_dataset(\"openai/gsm8k\", 'main')\n",
        "    dataset = dataset[\"train\"]\n",
        "    dataset = dataset.select(range(100))\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_dataset, val_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()  # Split 20% for validation\n",
        "\n",
        "    def tokenize_add_label(sample):\n",
        "        prompt = tokenizer.encode(tokenizer.bos_token + \"###Input:\\n\" + sample[\"question\"] + \"\\n\", add_special_tokens=False)\n",
        "        labels = tokenizer.encode(\"###Output:\\n\" + sample[\"answer\"] + tokenizer.eos_token, add_special_tokens=False)\n",
        "        sample = {\n",
        "            \"input_ids\": prompt + labels,\n",
        "            \"attention_mask\": [1] * (len(prompt) + len(labels)),\n",
        "            \"labels\": prompt + labels\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "    # Apply the tokenize_add_label function to each dataset\n",
        "    train_dataset = train_dataset.map(tokenize_add_label, remove_columns=list(train_dataset.features))\n",
        "    val_dataset = val_dataset.map(tokenize_add_label, remove_columns=list(val_dataset.features))\n",
        "\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "gEsLDRizVgNf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class train_configy:\n",
        "    model_name: str=\"openai-community/gpt2\"\n",
        "    tokenizer_name: str=None\n",
        "    run_validation: bool=True\n",
        "    batch_size_training: int=1\n",
        "    batching_strategy: str=\"packing\" #alternative: padding\n",
        "    context_length: int=128\n",
        "    gradient_accumulation_steps: int=1\n",
        "    gradient_clipping: bool = True\n",
        "    gradient_clipping_threshold: float = 1.0\n",
        "    num_epochs: int=3\n",
        "    max_train_step: int=0\n",
        "    max_eval_step: int=0\n",
        "    num_workers_dataloader: int=1\n",
        "    lr: float=1e-4\n",
        "    weight_decay: float=0.0\n",
        "    gamma: float= 0.85\n",
        "    seed: int=42\n",
        "    mixed_precision: bool=True\n",
        "    val_batch_size: int=1\n",
        "    output_dir: str = \"/content/test_model\"\n",
        "    save_model: bool = True\n",
        "    save_metrics: bool = True # saves training metrics to a json file for later plotting\n",
        "    flop_counter: bool = False # Enable flop counter to measure model throughput, can not be used with pytorch profiler at the same time.\n",
        "    flop_counter_start: int = 3 # The step to start profiling, default is 3, which means after 3 steps of warmup stage, the profiler will start to count flops.\n",
        "    use_profiler: bool = False # Enable pytorch profiler, can not be used with flop counter at the same time.\n",
        "    profiler_dir: str = \"/content/test_model\" # will be used if using profiler"
      ],
      "metadata": {
        "id": "xvdWi52vSr9U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = train_configy()"
      ],
      "metadata": {
        "id": "4797lc77T85b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train, dataset_val = get_preprocessed_dataset(\n",
        "    tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "bU9ER82pU03h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"--> Training Set Length = {len(dataset_train)}\")\n",
        "print(f\"--> Validation Set Length = {len(dataset_val)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cActJ-PrWGWc",
        "outputId": "0bf15d6d-d8a9-41ad-ad50-9e034ceb1c7d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Training Set Length = 90\n",
            "--> Validation Set Length = 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_config.batching_strategy == \"packing\":\n",
        "    dataset_train = ConcatDataset(dataset_train, chunk_size=train_config.context_length)\n",
        "\n",
        "train_dl_kwargs = get_dataloader_kwargs(train_config, dataset_train, tokenizer, \"train\")\n",
        "\n",
        "# Create DataLoaders for the training and validation dataset\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    num_workers=train_config.num_workers_dataloader,\n",
        "    pin_memory=True,\n",
        "    **train_dl_kwargs,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3f5u_TgU7QY",
        "outputId": "e88b2812-ceb9-4049-a4a3-0b1895ded108"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing dataset: 100%|██████████| 90/90 [00:00<00:00, 3587.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataloader))\n",
        "print(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amPywuEtW3qh",
        "outputId": "36a44d83-57e1-49f3-8c1f-6bc9d634d0f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7db552c19cc0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataloader = None\n",
        "if train_config.run_validation:\n",
        "    if train_config.batching_strategy == \"packing\":\n",
        "        dataset_val = ConcatDataset(dataset_val, chunk_size=train_config.context_length)\n",
        "\n",
        "    val_dl_kwargs = get_dataloader_kwargs(train_config, dataset_val, tokenizer, \"val\")\n",
        "\n",
        "    eval_dataloader = torch.utils.data.DataLoader(\n",
        "        dataset_val,\n",
        "        num_workers=train_config.num_workers_dataloader,\n",
        "        pin_memory=True,\n",
        "        **val_dl_kwargs,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XHNYFjEULRp",
        "outputId": "cada7160-da8f-4bdb-ba7e-1ba32e5f0c03"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing dataset: 100%|██████████| 10/10 [00:00<00:00, 1437.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=train_config.lr,\n",
        "        weight_decay=train_config.weight_decay,\n",
        "    )\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)"
      ],
      "metadata": {
        "id": "luk88EqFUp2F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training process\n",
        "results = train(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    eval_dataloader,\n",
        "    tokenizer,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    train_config.gradient_accumulation_steps,\n",
        "    train_config,\n",
        "    None,\n",
        ")\n",
        "[print(f'Key: {k}, Value: {v}') for k, v in results.items()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb4aypwuXOhE",
        "outputId": "af6aef7c-d869-447b-c50e-6b509cc41497"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/115 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Training Epoch: 1/3, step 0/115 completed (loss: 3.5776889324188232):   1%|\u001b[34m          \u001b[0m| 1/115 [00:01<01:57,  1.03s/it]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:01,  7.85it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:00, 13.41it/s]\u001b[A\n",
            "evaluating Epoch:  36%|\u001b[32m███▌      \u001b[0m| 5/14 [00:00<00:00, 16.11it/s]\u001b[A\n",
            "evaluating Epoch:  57%|\u001b[32m█████▋    \u001b[0m| 8/14 [00:00<00:00, 18.38it/s]\u001b[A\n",
            "evaluating Epoch:  71%|\u001b[32m███████▏  \u001b[0m| 10/14 [00:00<00:00, 18.37it/s]\u001b[A\n",
            "evaluating Epoch:  86%|\u001b[32m████████▌ \u001b[0m| 12/14 [00:00<00:00, 18.59it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 16.30it/s]\n",
            "Training Epoch: 1/3, step 50/115 completed (loss: 2.4937636852264404):  44%|\u001b[34m████▍     \u001b[0m| 51/115 [00:08<00:07,  8.63it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:01,  8.60it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:00, 14.52it/s]\u001b[A\n",
            "evaluating Epoch:  43%|\u001b[32m████▎     \u001b[0m| 6/14 [00:00<00:00, 17.98it/s]\u001b[A\n",
            "evaluating Epoch:  57%|\u001b[32m█████▋    \u001b[0m| 8/14 [00:00<00:00, 17.62it/s]\u001b[A\n",
            "evaluating Epoch:  71%|\u001b[32m███████▏  \u001b[0m| 10/14 [00:00<00:00, 15.99it/s]\u001b[A\n",
            "evaluating Epoch:  86%|\u001b[32m████████▌ \u001b[0m| 12/14 [00:00<00:00, 14.95it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:01<00:00, 12.33it/s]\n",
            "Training Epoch: 1/3, step 100/115 completed (loss: 2.2221696376800537):  88%|\u001b[34m████████▊ \u001b[0m| 101/115 [00:17<00:01,  7.06it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:02,  4.37it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:01,  9.34it/s]\u001b[A\n",
            "evaluating Epoch:  36%|\u001b[32m███▌      \u001b[0m| 5/14 [00:00<00:00, 12.40it/s]\u001b[A\n",
            "evaluating Epoch:  50%|\u001b[32m█████     \u001b[0m| 7/14 [00:00<00:00, 14.27it/s]\u001b[A\n",
            "evaluating Epoch:  64%|\u001b[32m██████▍   \u001b[0m| 9/14 [00:00<00:00, 15.30it/s]\u001b[A\n",
            "evaluating Epoch:  79%|\u001b[32m███████▊  \u001b[0m| 11/14 [00:00<00:00, 16.09it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:01<00:00, 13.21it/s]\n",
            "Training Epoch: 1/3, step 114/115 completed (loss: 2.8527681827545166): 100%|\u001b[34m██████████\u001b[0m| 115/115 [00:21<00:00,  8.01it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Training Epoch: 1/3, step 114/115 completed (loss: 2.8527681827545166): 100%|\u001b[34m██████████\u001b[0m| 115/115 [00:21<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max CUDA memory allocated was 2 GB\n",
            "Max CUDA memory reserved was 2 GB\n",
            "Peak active CUDA memory was 2 GB\n",
            "CUDA Malloc retries : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 24.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is saved in /content/test_model/epoch0 directory\n",
            "best eval loss on epoch 1 is 2.23980450630188\n",
            "Epoch 1: train_perplexity=13.6916, train_epoch_loss=2.6168, epoch time 21.769388803000027s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "Training Epoch: 2:   0%|\u001b[34m          \u001b[0m| 0/115 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Training Epoch: 2/3, step 0/115 completed (loss: 1.7043362855911255):   1%|\u001b[34m          \u001b[0m| 1/115 [00:00<00:19,  5.79it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:01,  7.39it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:00, 12.56it/s]\u001b[A\n",
            "evaluating Epoch:  36%|\u001b[32m███▌      \u001b[0m| 5/14 [00:00<00:00, 15.20it/s]\u001b[A\n",
            "evaluating Epoch:  50%|\u001b[32m█████     \u001b[0m| 7/14 [00:00<00:00, 16.69it/s]\u001b[A\n",
            "evaluating Epoch:  71%|\u001b[32m███████▏  \u001b[0m| 10/14 [00:00<00:00, 17.89it/s]\u001b[A\n",
            "evaluating Epoch:  86%|\u001b[32m████████▌ \u001b[0m| 12/14 [00:00<00:00, 18.06it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 15.48it/s]\n",
            "Training Epoch: 2/3, step 50/115 completed (loss: 1.7790257930755615):  44%|\u001b[34m████▍     \u001b[0m| 51/115 [00:08<00:07,  8.20it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:01,  8.63it/s]\u001b[A\n",
            "evaluating Epoch:  29%|\u001b[32m██▊       \u001b[0m| 4/14 [00:00<00:00, 17.57it/s]\u001b[A\n",
            "evaluating Epoch:  43%|\u001b[32m████▎     \u001b[0m| 6/14 [00:00<00:00, 17.27it/s]\u001b[A\n",
            "evaluating Epoch:  64%|\u001b[32m██████▍   \u001b[0m| 9/14 [00:00<00:00, 19.05it/s]\u001b[A\n",
            "evaluating Epoch:  79%|\u001b[32m███████▊  \u001b[0m| 11/14 [00:00<00:00, 18.39it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 16.91it/s]\n",
            "Training Epoch: 2/3, step 100/115 completed (loss: 1.5589760541915894):  88%|\u001b[34m████████▊ \u001b[0m| 101/115 [00:15<00:01,  8.43it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:01,  8.37it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:00, 14.59it/s]\u001b[A\n",
            "evaluating Epoch:  43%|\u001b[32m████▎     \u001b[0m| 6/14 [00:00<00:00, 17.06it/s]\u001b[A\n",
            "evaluating Epoch:  57%|\u001b[32m█████▋    \u001b[0m| 8/14 [00:00<00:00, 17.63it/s]\u001b[A\n",
            "evaluating Epoch:  79%|\u001b[32m███████▊  \u001b[0m| 11/14 [00:00<00:00, 18.61it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 16.67it/s]\n",
            "Training Epoch: 2/3, step 114/115 completed (loss: 2.1792731285095215): 100%|\u001b[34m██████████\u001b[0m| 115/115 [00:19<00:00,  7.49it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Training Epoch: 2/3, step 114/115 completed (loss: 2.1792731285095215): 100%|\u001b[34m██████████\u001b[0m| 115/115 [00:19<00:00,  5.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max CUDA memory allocated was 2 GB\n",
            "Max CUDA memory reserved was 2 GB\n",
            "Peak active CUDA memory was 2 GB\n",
            "CUDA Malloc retries : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 19.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is saved in /content/test_model/epoch1 directory\n",
            "Epoch 2: train_perplexity=5.7781, train_epoch_loss=1.7541, epoch time 20.222684768000022s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "Training Epoch: 3:   0%|\u001b[34m          \u001b[0m| 0/115 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Training Epoch: 3/3, step 0/115 completed (loss: 1.2424379587173462):   1%|\u001b[34m          \u001b[0m| 1/115 [00:00<00:20,  5.60it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:01,  7.89it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:00, 12.69it/s]\u001b[A\n",
            "evaluating Epoch:  36%|\u001b[32m███▌      \u001b[0m| 5/14 [00:00<00:00, 14.79it/s]\u001b[A\n",
            "evaluating Epoch:  50%|\u001b[32m█████     \u001b[0m| 7/14 [00:00<00:00, 16.08it/s]\u001b[A\n",
            "evaluating Epoch:  71%|\u001b[32m███████▏  \u001b[0m| 10/14 [00:00<00:00, 18.03it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 16.11it/s]\n",
            "Training Epoch: 3/3, step 50/115 completed (loss: 1.3549705743789673):  44%|\u001b[34m████▍     \u001b[0m| 51/115 [00:07<00:08,  7.91it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:02,  6.00it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "evaluating Epoch:  36%|\u001b[32m███▌      \u001b[0m| 5/14 [00:00<00:00, 14.15it/s]\u001b[A\n",
            "evaluating Epoch:  57%|\u001b[32m█████▋    \u001b[0m| 8/14 [00:00<00:00, 17.11it/s]\u001b[A\n",
            "evaluating Epoch:  71%|\u001b[32m███████▏  \u001b[0m| 10/14 [00:00<00:00, 16.31it/s]\u001b[A\n",
            "evaluating Epoch:  86%|\u001b[32m████████▌ \u001b[0m| 12/14 [00:00<00:00, 16.39it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:01<00:00, 13.59it/s]\n",
            "Training Epoch: 3/3, step 100/115 completed (loss: 1.086852788925171):  88%|\u001b[34m████████▊ \u001b[0m| 101/115 [00:15<00:01,  8.72it/s]\n",
            "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "evaluating Epoch:   7%|\u001b[32m▋         \u001b[0m| 1/14 [00:00<00:01,  9.33it/s]\u001b[A\n",
            "evaluating Epoch:  21%|\u001b[32m██▏       \u001b[0m| 3/14 [00:00<00:00, 14.54it/s]\u001b[A\n",
            "evaluating Epoch:  36%|\u001b[32m███▌      \u001b[0m| 5/14 [00:00<00:00, 15.40it/s]\u001b[A\n",
            "evaluating Epoch:  57%|\u001b[32m█████▋    \u001b[0m| 8/14 [00:00<00:00, 18.16it/s]\u001b[A\n",
            "evaluating Epoch:  79%|\u001b[32m███████▊  \u001b[0m| 11/14 [00:00<00:00, 19.26it/s]\u001b[A\n",
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 17.10it/s]\n",
            "Training Epoch: 3/3, step 114/115 completed (loss: 1.6364105939865112): 100%|\u001b[34m██████████\u001b[0m| 115/115 [00:19<00:00,  7.92it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Training Epoch: 3/3, step 114/115 completed (loss: 1.6364105939865112): 100%|\u001b[34m██████████\u001b[0m| 115/115 [00:19<00:00,  6.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max CUDA memory allocated was 2 GB\n",
            "Max CUDA memory reserved was 2 GB\n",
            "Peak active CUDA memory was 2 GB\n",
            "CUDA Malloc retries : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 24.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is saved in /content/test_model/epoch2 directory\n",
            "Epoch 3: train_perplexity=3.8314, train_epoch_loss=1.3432, epoch time 19.770632068999987s\n",
            "Key: avg_train_prep, Value: 7.767048199971517\n",
            "Key: avg_train_loss, Value: 1.9046973784764607\n",
            "Key: avg_eval_prep, Value: 10.426742553710938\n",
            "Key: avg_eval_loss, Value: 2.23980450630188\n",
            "Key: avg_epoch_time, Value: 20.587568546666677\n",
            "Key: avg_checkpoint_time, Value: 2.593422291333335\n",
            "Key: metrics_filename, Value: /content/test_model/metrics_data_None-2024-09-19_23-06-35.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inference from Trained Model"
      ],
      "metadata": {
        "id": "R9os-rHUezqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "IrI4xSN5a8ra"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/content/test_model/epoch2\",\n",
        "    device_map=\"cuda\",\n",
        "    use_cache=None,\n",
        "    attn_implementation=None,\n",
        ")"
      ],
      "metadata": {
        "id": "AgFexcDDe8rD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = '''What is the sum of 20 + 30?'''\n",
        "batch = tokenizer(user_prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "c54ceAlGfIDi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **batch,\n",
        "        max_new_tokens=50, #The maximum numbers of tokens to generate\n",
        "        do_sample=False, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
        "        top_p=1.0, # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
        "        temperature=0, # [optional] The value used to modulate the next token probabilities.\n",
        "        min_length=None, #The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
        "        use_cache=True, #[optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
        "        top_k=50, # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "        repetition_penalty=1.0, #The parameter for repetition penalty. 1.0 means no penalty.\n",
        "        length_penalty=1, #[optional] Exponential penalty to the length that is used with beam-based generation.\n",
        "        output_hidden_states= True, return_dict_in_generate=True,\n",
        "    )\n",
        "batch = {k: v.to(\"cpu\") for k, v in batch.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI3BAghRfUxB",
        "outputId": "d78e3645-4859-40d9-aa09-f43ad3f21dd1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "print(\"The complete decode:\",output_text)\n",
        "print(\"--------------------------------\")\n",
        "print(\"Just the model's response\",output_text[len(user_prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afO68NwOfVja",
        "outputId": "f6fc47dc-28a7-49d4-bdab-080d9b96cc3b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The complete decode: What is the sum of 20 + 30?\n",
            "###Output:\n",
            "James earns $20 an hour while working his shift.  He earns $15 an hour while working his shift.  How much does he earn per week?\n",
            "###Output:\n",
            "James earns $20 an hour while working\n",
            "--------------------------------\n",
            "Just the model's response \n",
            "###Output:\n",
            "James earns $20 an hour while working his shift.  He earns $15 an hour while working his shift.  How much does he earn per week?\n",
            "###Output:\n",
            "James earns $20 an hour while working\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NSLSNZrfdt1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZ_OFWCEfdiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}